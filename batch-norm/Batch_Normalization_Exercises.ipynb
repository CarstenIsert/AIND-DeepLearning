{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch Normalization – Practice\n",
    "\n",
    "### Solution by Carsten Isert, Sept. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Batch normalization is most useful when building deep neural networks. To demonstrate this, we'll create a convolutional neural network with 20 convolutional layers, followed by a fully connected layer. We'll use it to classify handwritten digits in the MNIST dataset, which should be familiar to you by now.\n",
    "\n",
    "This is **not** a good network for classfying MNIST digits. You could create a _much_ simpler network and get _better_ results. However, to give you hands-on experience with batch normalization, we had to make an example that was:\n",
    "1. Complicated enough that training would benefit from batch normalization.\n",
    "2. Simple enough that it would train quickly, since this is meant to be a short exercise just to give you some practice adding batch normalization.\n",
    "3. Simple enough that the architecture would be easy to understand without additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook includes two versions of the network that you can edit. The first uses higher level functions from the `tf.layers` package. The second is the same network, but uses only lower level functions in the `tf.nn` package.\n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1)\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell loads TensorFlow, downloads the MNIST dataset if necessary, and loads it into an object named `mnist`. You'll need to run this cell before running anything else in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch Normalization using `tf.layers.batch_normalization`<a id=\"example_1\"></a>\n",
    "\n",
    "This version of the network uses `tf.layers` for almost everything, and expects you to implement batch normalization using [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll use the following function to create fully connected layers in our network. We'll create them with the specified number of neurons and a ReLU activation function.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll use the following function to create convolutional layers in our network. They are very basic: we're always using a 3x3 kernel, ReLU activation functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We aren't bothering with pooling layers at all in this network.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Run the following cell**, along with the earlier cells (to load the dataset and define the necessary functions). \n",
    "\n",
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69119, Validation accuracy: 0.11260\n",
      "Batch: 25: Training loss: 0.33375, Training accuracy: 0.07812\n",
      "Batch: 50: Training loss: 0.32556, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.32628, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.32533, Validation accuracy: 0.10700\n",
      "Batch: 125: Training loss: 0.32603, Training accuracy: 0.03125\n",
      "Batch: 150: Training loss: 0.32553, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.32363, Training accuracy: 0.09375\n",
      "Batch: 200: Validation loss: 0.32575, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.32482, Training accuracy: 0.15625\n",
      "Batch: 250: Training loss: 0.32652, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32345, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32576, Validation accuracy: 0.12380\n",
      "Batch: 325: Training loss: 0.32276, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.32308, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32698, Training accuracy: 0.09375\n",
      "Batch: 400: Validation loss: 0.32495, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 0.32571, Training accuracy: 0.09375\n",
      "Batch: 450: Training loss: 0.32673, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32539, Training accuracy: 0.04688\n",
      "Batch: 500: Validation loss: 0.32523, Validation accuracy: 0.10020\n",
      "Batch: 525: Training loss: 0.32794, Training accuracy: 0.12500\n",
      "Batch: 550: Training loss: 0.32352, Training accuracy: 0.20312\n",
      "Batch: 575: Training loss: 0.32264, Training accuracy: 0.20312\n",
      "Batch: 600: Validation loss: 0.32527, Validation accuracy: 0.06280\n",
      "Batch: 625: Training loss: 0.32485, Training accuracy: 0.14062\n",
      "Batch: 650: Training loss: 0.32803, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32535, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32528, Validation accuracy: 0.09860\n",
      "Batch: 725: Training loss: 0.32480, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32520, Training accuracy: 0.14062\n",
      "Batch: 775: Training loss: 0.32546, Training accuracy: 0.14062\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With this many layers, it's going to take a lot of iterations for this network to learn. By the time you're done training these 800 batches, your final test and validation accuracies probably won't be much better than 10%. (It will be different each time, but will most likely be less than 15%.)\n",
    "\n",
    "Using batch normalization, you'll be able to train this same network to over 90% in that same number of batches.\n",
    "\n",
    "\n",
    "# Add batch normalization\n",
    "\n",
    "We've copied the previous three cells to get you started. **Edit these cells** to add batch normalization to the network. For this exercise, you should use [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) to handle most of the math, but you'll need to make a few other changes to your network to integrate batch normalization. You may want to refer back to the lesson notebook to remind yourself of important things, like how your graph operations need to know whether or not you are performing training or inference. \n",
    "\n",
    "If you get stuck, you can check out the `Batch_Normalization_Solutions` notebook to see how we did things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Modified `fully_connected` to add batch normalization to the fully connected layers it creates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: Bool\n",
    "        Indicates if the network is currently in training mode (True) or in inference mode (False)\n",
    "        If we are in training mode we need to update the population statistics and use them in the\n",
    "        inference stage\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    # As batch normalization uses additional parameters gamma and beta we don't need the biases and\n",
    "    # must deactivate them. The activation function must be added after the batch normalization step\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Modified `conv_layer` to add batch normalization to the convolutional layers it creates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: Bool\n",
    "        Indicates if the network is currently in training mode (True) or in inference mode (False)\n",
    "        If we are in training mode we need to update the population statistics and use them in the\n",
    "        inference stage\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=None, use_bias=False)\n",
    "    # As described in the lesson, we need to do the batch noramlization on the filters of the convolution\n",
    "    # As it turns out, this should work automatically in this case without specifying the axis explicitly.\n",
    "    # The activation function must be added after the batch normalization and the layer must know if it\n",
    "    # is training or not to use the population parameters for inference\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Edited the `train` function to support batch normalization. \n",
    "\n",
    "Need to make sure the network knows whether or not it is training, and need to make sure it updates and uses its population statistics correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69147, Validation accuracy: 0.11000\n",
      "Batch: 25: Training loss: 0.58453, Training accuracy: 0.10938\n",
      "Batch: 50: Training loss: 0.47564, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.40835, Training accuracy: 0.01562\n",
      "Batch: 100: Validation loss: 0.36341, Validation accuracy: 0.09900\n",
      "Batch: 125: Training loss: 0.34548, Training accuracy: 0.14062\n",
      "Batch: 150: Training loss: 0.34283, Training accuracy: 0.09375\n",
      "Batch: 175: Training loss: 0.32770, Training accuracy: 0.17188\n",
      "Batch: 200: Validation loss: 0.40074, Validation accuracy: 0.11360\n",
      "Batch: 225: Training loss: 0.38259, Training accuracy: 0.18750\n",
      "Batch: 250: Training loss: 0.34966, Training accuracy: 0.31250\n",
      "Batch: 275: Training loss: 0.30710, Training accuracy: 0.46875\n",
      "Batch: 300: Validation loss: 0.22064, Validation accuracy: 0.64000\n",
      "Batch: 325: Training loss: 0.08196, Training accuracy: 0.87500\n",
      "Batch: 350: Training loss: 0.16781, Training accuracy: 0.71875\n",
      "Batch: 375: Training loss: 0.06165, Training accuracy: 0.85938\n",
      "Batch: 400: Validation loss: 0.07419, Validation accuracy: 0.87220\n",
      "Batch: 425: Training loss: 0.04681, Training accuracy: 0.92188\n",
      "Batch: 450: Training loss: 0.04306, Training accuracy: 0.95312\n",
      "Batch: 475: Training loss: 0.05972, Training accuracy: 0.92188\n",
      "Batch: 500: Validation loss: 0.03838, Validation accuracy: 0.95060\n",
      "Batch: 525: Training loss: 0.05064, Training accuracy: 0.92188\n",
      "Batch: 550: Training loss: 0.05546, Training accuracy: 0.92188\n",
      "Batch: 575: Training loss: 0.00760, Training accuracy: 0.98438\n",
      "Batch: 600: Validation loss: 0.05357, Validation accuracy: 0.91580\n",
      "Batch: 625: Training loss: 0.04881, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.01582, Training accuracy: 0.96875\n",
      "Batch: 675: Training loss: 0.05477, Training accuracy: 0.92188\n",
      "Batch: 700: Validation loss: 0.02817, Validation accuracy: 0.95900\n",
      "Batch: 725: Training loss: 0.03587, Training accuracy: 0.93750\n",
      "Batch: 750: Training loss: 0.03191, Training accuracy: 0.96875\n",
      "Batch: 775: Training loss: 0.06887, Training accuracy: 0.92188\n",
      "Final validation accuracy: 0.96540\n",
      "Final test accuracy: 0.96440\n",
      "Accuracy on 100 samples: 1.0\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    \n",
    "    # Build the network with 20 layers\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    # If we don't include the update ops as dependencies on the train step, the \n",
    "    # tf.layers.batch_normalization layers won't update their population statistics,\n",
    "    # which will cause the model to fail at inference time\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With batch normalization, you should now get an accuracy over 90%. Notice also the last line of the output: `Accuracy on 100 samples`. If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference.\n",
    "\n",
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature – something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM – then you may need to know these sorts of things.\n",
    "\n",
    "This version of the network uses `tf.nn` for almost everything, and expects you to implement batch normalization using [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "Modified `fully_connected` to add batch normalization to the fully connected layers it creates. \n",
    "\n",
    "**Note:** For convenience, we continue to use `tf.layers.dense` for the `fully_connected` layer. By this point in the class, you should have no problem replacing that with matrix operations between the `prev_layer` and explicit weights and biases variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: Bool\n",
    "        Indicates if the network is currently in training mode (True) or in inference mode (False)\n",
    "        If we are in training mode we need to update the population statistics and use them in the\n",
    "        inference stage\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_output = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "\n",
    "    # Batch normalization adds additional trainable variables: \n",
    "    # gamma (for scaling) and beta (for shifting).\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "\n",
    "    # These variables will store the mean and variance for this layer over the entire training set,\n",
    "    # which we assume represents the general population distribution.\n",
    "    # By setting `trainable=False`, we tell TensorFlow not to modify these variables during\n",
    "    # back propagation. Instead, we will assign values to these variables ourselves. \n",
    "    population_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "    \n",
    "    # Batch normalization requires a small constant epsilon, used to ensure we don't divide by zero.\n",
    "    # This is the default value TensorFlow uses.\n",
    "    epsilon = 1e-3\n",
    "\n",
    "    def batch_norm_training():\n",
    "        # Calculate the mean and variance for the data coming out of this layer's linear-combination step.\n",
    "        # The [0] defines an array of axes to calculate over.\n",
    "        batch_mean, batch_variance = tf.nn.moments(linear_output, [0])\n",
    "\n",
    "        # Calculate a moving average of the training data's mean and variance while training.\n",
    "        # These will be used during inference.\n",
    "        # Decay should be some number less than 1. tf.layers.batch_normalization uses the parameter\n",
    "        # \"momentum\" to accomplish this and defaults it to 0.99\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        # The 'tf.control_dependencies' context tells TensorFlow it must calculate 'train_mean' \n",
    "        # and 'train_variance' before it calculates the 'tf.nn.batch_normalization' layer.\n",
    "        # This is necessary because the those two operations are not actually in the graph\n",
    "        # connecting the linear_output and batch_normalization layers, \n",
    "        # so TensorFlow would otherwise just skip them.\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(linear_output, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        # During inference, use the our estimated population mean and variance to normalize the layer\n",
    "        return tf.nn.batch_normalization(linear_output, population_mean, population_variance, beta, gamma, epsilon)\n",
    "\n",
    "    # Use `tf.cond` as a sort of if-check. When is_training is True, TensorFlow will execute \n",
    "    # the operation returned from `batch_norm_training`; otherwise it will execute the graph\n",
    "    # operation returned from `batch_norm_inference`.\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    \n",
    "    layer = tf.nn.relu(batch_normalized_output)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Modified `conv_layer` to add batch normalization to the fully connected layers it creates. \n",
    "\n",
    "**Note:** Unlike in the previous example that used `tf.layers`, adding batch normalization to these convolutional layers _does_ require some slight differences to what you did in `fully_connected`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: Bool\n",
    "        Indicates if the network is currently in training mode (True) or in inference mode (False)\n",
    "        If we are in training mode we need to update the population statistics and use them in the\n",
    "        inference stage\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "\n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "\n",
    "    # Batch normalization adds additional trainable variables: \n",
    "    # gamma (for scaling) and beta (for shifting).\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "\n",
    "    # These variables will store the mean and variance for this layer over the entire training set,\n",
    "    # which we assume represents the general population distribution.\n",
    "    # By setting `trainable=False`, we tell TensorFlow not to modify these variables during\n",
    "    # back propagation. Instead, we will assign values to these variables ourselves. \n",
    "    population_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "    \n",
    "    # Batch normalization requires a small constant epsilon, used to ensure we don't divide by zero.\n",
    "    # This is the default value TensorFlow uses.\n",
    "    epsilon = 1e-3\n",
    "\n",
    "    def batch_norm_training():\n",
    "        # Important to use the correct dimensions here to ensure the mean and variance are calculated \n",
    "        # per feature map instead of for the entire layer\n",
    "        # The axis description is taken directly from the tf.nn.momnents API documentation\n",
    "        batch_mean, batch_variance = tf.nn.moments(conv_layer, [0,1,2], keep_dims=False)\n",
    "\n",
    "        # Calculate a moving average of the training data's mean and variance while training.\n",
    "        # These will be used during inference.\n",
    "        # Decay should be some number less than 1. tf.layers.batch_normalization uses the parameter\n",
    "        # \"momentum\" to accomplish this and defaults it to 0.99\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        # The 'tf.control_dependencies' context tells TensorFlow it must calculate 'train_mean' \n",
    "        # and 'train_variance' before it calculates the 'tf.nn.batch_normalization' layer.\n",
    "        # This is necessary because the those two operations are not actually in the graph\n",
    "        # connecting the linear_output and batch_normalization layers, \n",
    "        # so TensorFlow would otherwise just skip them.\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(conv_layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        # During inference, use the our estimated population mean and variance to normalize the layer\n",
    "        return tf.nn.batch_normalization(conv_layer, population_mean, population_variance, beta, gamma, epsilon)\n",
    "\n",
    "    # Use `tf.cond` as a sort of if-check. When is_training is True, TensorFlow will execute \n",
    "    # the operation returned from `batch_norm_training`; otherwise it will execute the graph\n",
    "    # operation returned from `batch_norm_inference`.\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    \n",
    "    conv_layer = tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Edited the `train` function to support batch normalization. Basically the same as in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69160, Validation accuracy: 0.09580\n",
      "Batch: 25: Training loss: 0.59775, Training accuracy: 0.12500\n",
      "Batch: 50: Training loss: 0.49674, Training accuracy: 0.06250\n",
      "Batch: 75: Training loss: 0.42317, Training accuracy: 0.09375\n",
      "Batch: 100: Validation loss: 0.37380, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.35883, Training accuracy: 0.09375\n",
      "Batch: 150: Training loss: 0.34865, Training accuracy: 0.06250\n",
      "Batch: 175: Training loss: 0.33896, Training accuracy: 0.12500\n",
      "Batch: 200: Validation loss: 0.41505, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.52838, Training accuracy: 0.07812\n",
      "Batch: 250: Training loss: 0.64349, Training accuracy: 0.06250\n",
      "Batch: 275: Training loss: 0.80759, Training accuracy: 0.07812\n",
      "Batch: 300: Validation loss: 0.97752, Validation accuracy: 0.11260\n",
      "Batch: 325: Training loss: 0.62979, Training accuracy: 0.12500\n",
      "Batch: 350: Training loss: 0.59389, Training accuracy: 0.25000\n",
      "Batch: 375: Training loss: 0.82534, Training accuracy: 0.14062\n",
      "Batch: 400: Validation loss: 0.41274, Validation accuracy: 0.43100\n",
      "Batch: 425: Training loss: 0.16004, Training accuracy: 0.76562\n",
      "Batch: 450: Training loss: 0.23573, Training accuracy: 0.73438\n",
      "Batch: 475: Training loss: 0.16954, Training accuracy: 0.75000\n",
      "Batch: 500: Validation loss: 0.10304, Validation accuracy: 0.84140\n",
      "Batch: 525: Training loss: 0.07870, Training accuracy: 0.82812\n",
      "Batch: 550: Training loss: 0.02497, Training accuracy: 0.95312\n",
      "Batch: 575: Training loss: 0.03467, Training accuracy: 0.93750\n",
      "Batch: 600: Validation loss: 0.06293, Validation accuracy: 0.92220\n",
      "Batch: 625: Training loss: 0.11389, Training accuracy: 0.85938\n",
      "Batch: 650: Training loss: 0.01713, Training accuracy: 0.96875\n",
      "Batch: 675: Training loss: 0.03701, Training accuracy: 0.93750\n",
      "Batch: 700: Validation loss: 0.04032, Validation accuracy: 0.94040\n",
      "Batch: 725: Training loss: 0.02346, Training accuracy: 0.95312\n",
      "Batch: 750: Training loss: 0.02658, Training accuracy: 0.96875\n",
      "Batch: 775: Training loss: 0.05728, Training accuracy: 0.93750\n",
      "Final validation accuracy: 0.97160\n",
      "Final test accuracy: 0.97380\n",
      "Accuracy on 100 samples: 0.99\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    # If we don't include the update ops as dependencies on the train step, the \n",
    "    # tf.layers.batch_normalization layers won't update their population statistics,\n",
    "    # which will cause the model to fail at inference time\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
